{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Home Price - Fine-Tune Base Model \n",
    "\n",
    "### *Introduction*  \n",
    "The goal of this project is to makes predictions about the future sale prices of homes. The prediction results are evaluated on **Mean Absolute Error** between the predicted log error and the actual log error. The logerror (target variable) is defined as ***logerror=log(Zestimate)âˆ’log(SalePrice)*** and is recorded in the training data. \n",
    "\n",
    "In the previous notebook, the data which contains the list of real estate properties in three counties (Los Angeles, Orange and Ventura, California) data in 2016 was analyzed and cleaned. It was found that the dataset contains both numerical and categorical features, the correlation coefficients between the features and target variable are relatively small, and also a large portion of the data are missing. Base models were built with the cleaned dataset and were compared. Ridge model, RandomForestRegressor, and GradientBoostingRegressor show similar cross validation scores. \n",
    "\n",
    "### *About This Notebook*\n",
    "This notebook focuses on **Base model fine-tuning**, i.e., use GridSearchCV to fine tune the model hyperparameters.  \n",
    "  \n",
    "Cross validation scores of optimized models:\n",
    "- Ridge: mean -0.053128 (std 0.000603)\n",
    "- KNN: mean -0.053374 (std 0.000624)\n",
    "- RF: mean -0.053026 (std 0.000639)\n",
    "- GB: mean -0.052970 (std 0.000647)\n",
    "\n",
    "***Next Step:***  \n",
    "The next step is to stack these fine-tuned base models and to evaluate the performance of the stacked models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0 | Package and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import seaborn as sns\n",
    "sns.set_style('white')\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initiate random seed\n",
    "SEED = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hide warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 | Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DATA_PATH = '../data'\n",
    "\n",
    "def load_data(path, file_name):\n",
    "    \"\"\"load csv data and return dataframe\"\"\"\n",
    "    csv_path = os.path.join(path, file_name)\n",
    "    return pd.read_csv(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load train_merge.csv - created in Notebook1\n",
    "train_merge = load_data(DATA_PATH, file_name='train_merge.csv')\n",
    "# drop `transactiondate` and `parcelid` in train\n",
    "train_merge.drop(['transactiondate', 'parcelid'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load prop_downsized.csv - created in Notebook1\n",
    "prop = load_data(DATA_PATH, file_name='prop_downsized.csv')\n",
    "# drop `parcelid` in prop\n",
    "prop.drop(['parcelid'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: (72220, 58)\n",
      "Test set size: (18055, 58)\n"
     ]
    }
   ],
   "source": [
    "# set aside a test set\n",
    "train_set, test_set = train_test_split(train_merge, test_size=0.2, random_state=SEED)\n",
    "print('Training set size: {}\\nTest set size: {}'.format(train_set.shape, test_set.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 | Model Fine-Tune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Transformer Class and Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class DataFrameSelector(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"select desired features and drop the rest\"\"\"\n",
    "    def __init__(self, features):\n",
    "        self.features = features\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        return X[self.features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FeatureAdder(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"add new features including average size of rooms, ratio between living area and lot size,\n",
    "    ratio between property tax and total tax, and ratio between structure value and land value\"\"\"\n",
    "    def __init__(self, add_new_feature=True):\n",
    "        self.add_new_feature = add_new_feature\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        # define new features\n",
    "        N_AvgSize = X['calculatedfinishedsquarefeet']/(X['bedroomcnt'] + X['bathroomcnt'] + 1)\n",
    "        N_PropLot = X['calculatedfinishedsquarefeet']/X['lotsizesquarefeet']\n",
    "        N_ValueRatio = X['taxamount']/X['taxvaluedollarcnt']\n",
    "        N_StructLand = X['structuretaxvaluedollarcnt']/X['landtaxvaluedollarcnt']\n",
    "        # add new features if True\n",
    "        if self.add_new_feature:\n",
    "            return np.c_[X, N_AvgSize, N_PropLot, N_ValueRatio, N_StructLand]\n",
    "        else:\n",
    "            return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FeatureDropper(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"drop features with percentage of missing missing values larger than missing_pct\"\"\"\n",
    "    def __init__(self, missing_pct=1, drop_cols=[]):\n",
    "        self.missing_pct = missing_pct # missing value percentage threshold\n",
    "        self.drop_cols = drop_cols # initialize columns to drop\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        for col in X.columns:\n",
    "            if pd.isnull(X[col]).sum()/X.shape[0] >= self.missing_pct:\n",
    "                self.drop_cols.append(col)\n",
    "        X = X.drop(self.drop_cols, axis=1)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class CatTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"categorival feature transformer: impute categorical value and encode categories\"\"\"\n",
    "    def __init__(self, cat_dict):\n",
    "        self.cat_dict = cat_dict\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        for col in X.columns:\n",
    "            if X[col].dtype == 'O':\n",
    "                X[col].fillna('-99', inplace=True)\n",
    "            X[col].fillna(-99, inplace=True)\n",
    "            X[col] = X[col].astype('category', categories=self.cat_dict[col])\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DummyEncoder(TransformerMixin):\n",
    "    \"\"\"create dummy variables\"\"\"\n",
    "    def __init__(self, columns=None):\n",
    "        self.columns = columns\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        return pd.get_dummies(X, columns=self.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# numerical features\n",
    "num_features = ['basementsqft', 'bathroomcnt', 'bedroomcnt', 'calculatedbathnbr', \\\n",
    "             'threequarterbathnbr', 'finishedfloor1squarefeet', 'calculatedfinishedsquarefeet',\\\n",
    "             'finishedsquarefeet6', 'finishedsquarefeet12', 'finishedsquarefeet13', \\\n",
    "             'finishedsquarefeet15', 'finishedsquarefeet50', 'fireplacecnt', 'fullbathcnt', \\\n",
    "             'garagecarcnt', 'garagetotalsqft', 'latitude', 'longitude', 'lotsizesquarefeet', \\\n",
    "             'numberofstories', 'poolcnt', 'poolsizesum', 'roomcnt', \\\n",
    "             'unitcnt', 'yardbuildingsqft17', 'yardbuildingsqft26', 'yearbuilt', \\\n",
    "             'taxvaluedollarcnt', 'structuretaxvaluedollarcnt', 'landtaxvaluedollarcnt', \\\n",
    "             'taxamount', 'assessmentyear']\n",
    "\n",
    "# categorical features\n",
    "cat_features = ['airconditioningtypeid', 'decktypeid', 'architecturalstyletypeid', \\\n",
    "               'buildingclasstypeid', 'heatingorsystemtypeid', 'fips', 'fireplaceflag', \\\n",
    "               'hashottuborspa', 'pooltypeid10', 'pooltypeid2', 'propertylandusetypeid', \\\n",
    "               'propertyzoningdesc', 'regionidcounty', 'taxdelinquencyflag', 'propertycountylandusecode', \\\n",
    "                'rawcensustractandblock', 'censustractandblock', 'regionidcity', 'regionidzip', \\\n",
    "                'regionidneighborhood', 'storytypeid', 'pooltypeid7', 'typeconstructiontypeid', 'taxdelinquencyyear']\n",
    "\n",
    "# potential features to drop (categorical variables with large number of levels) \n",
    "drop_features = ['propertycountylandusecode', 'rawcensustractandblock', 'censustractandblock', \\\n",
    "                 'regionidcity', 'regionidzip', 'regionidneighborhood', 'propertyzoningdesc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# levels of categorical variables\n",
    "categories = {}\n",
    "for col in cat_features:\n",
    "    if prop[col].dtype == 'O':\n",
    "        prop[col].fillna('-99', inplace=True)\n",
    "    prop[col].fillna(-99, inplace=True)\n",
    "    categories[col] = prop[col].astype('category').cat.categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save the categorical variables for future use\n",
    "with open('../data/categories.pickle', 'wb') as handle:\n",
    "    pickle.dump(categories, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('../data/categories.pickle', 'rb') as handle:\n",
    "    categories = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# pipeline for numerical features\n",
    "num_pipe = Pipeline([\n",
    "        ('selector', DataFrameSelector(num_features)),\n",
    "        ('feature_dropper', FeatureDropper(missing_pct=0.95)),\n",
    "        ('feature_adder', FeatureAdder(add_new_feature=True)),\n",
    "        ('imputer', Imputer(strategy='median')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# pipeline for categorical features\n",
    "cat_pipe = Pipeline([\n",
    "        ('selector', DataFrameSelector(cat_features)), \n",
    "        ('feature_dropper', FeatureDropper(missing_pct=0.95, drop_cols=drop_features)),\n",
    "        ('cat_transform', CatTransformer(categories)),\n",
    "        ('get_dummy', DummyEncoder())\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# full pipeline combining pipelines for numerical and categorical features\n",
    "full_pipe = FeatureUnion([\n",
    "        ('num_pipeline', num_pipe),\n",
    "        ('cat_pipeline', cat_pipe)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_wo_outlier = train_set[(train_set.logerror > -0.4) & (train_set.logerror < 0.42)]\n",
    "train_w_outlier = train_set.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels = train_wo_outlier['logerror'].values\n",
    "features = train_wo_outlier.drop(['logerror'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# transform features with full_pipe\n",
    "features_transformed = full_pipe.fit_transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# naive predictor - median predictor\n",
    "class NaivePredictor(BaseEstimator):\n",
    "    def fit(self, X, y):\n",
    "        pass\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return np.full((len(X), 1), 0.107)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
       "   normalize=False, random_state=8, solver='auto', tol=0.001),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'alpha': [0, 0.1, 1, 5, 10, 100, 1000, 10000, 100000]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='neg_mean_absolute_error', verbose=0)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# grid search for ridge\n",
    "ridge = Ridge(random_state=SEED)\n",
    "param_ridge = {'alpha': [0, 0.1, 1, 5, 10, 100, 1000, 10000, 100000]}\n",
    "\n",
    "grid_ridge = GridSearchCV(ridge, param_grid=param_ridge, cv=5,\n",
    "                         scoring='neg_mean_absolute_error')\n",
    "grid_ridge.fit(features_transformed, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73184877.9651 {'alpha': 0}\n",
      "0.0532292958466 {'alpha': 0.1}\n",
      "0.0532266417536 {'alpha': 1}\n",
      "0.0532236846688 {'alpha': 5}\n",
      "0.0532228450081 {'alpha': 10}\n",
      "0.0532063875374 {'alpha': 100}\n",
      "0.0531570333955 {'alpha': 1000}\n",
      "0.0531468038533 {'alpha': 10000}\n",
      "0.0531980478795 {'alpha': 100000}\n"
     ]
    }
   ],
   "source": [
    "cv_ridge = grid_ridge.cv_results_\n",
    "for mean_score, params in zip(cv_ridge['mean_test_score'], \n",
    "                              cv_ridge['params']):\n",
    "    print(-mean_score, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0531427412412 {'alpha': 8000}\n",
      "0.0531448051647 {'alpha': 9000}\n",
      "0.0531468038533 {'alpha': 10000}\n",
      "0.0531487035324 {'alpha': 11000}\n",
      "0.053150534403 {'alpha': 12000}\n",
      "0.0531523167158 {'alpha': 13000}\n"
     ]
    }
   ],
   "source": [
    "# fine tune\n",
    "param_ridge2 = {'alpha': [8000, 9000, 10000, 11000, 12000, 13000]}\n",
    "\n",
    "grid_ridge2 = GridSearchCV(ridge, param_grid=param_ridge2, cv=5,\n",
    "                         scoring='neg_mean_absolute_error')\n",
    "grid_ridge2.fit(features_transformed, labels)\n",
    "\n",
    "cv_ridge2 = grid_ridge2.cv_results_\n",
    "for mean_score, params in zip(cv_ridge2['mean_test_score'], \n",
    "                              cv_ridge2['params']):\n",
    "    print(-mean_score, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0531423855228 {'alpha': 2000}\n",
      "0.0531371518153 {'alpha': 3000}\n",
      "0.0531364293486 {'alpha': 4000}\n",
      "0.0531372551883 {'alpha': 5000}\n",
      "0.0531386986655 {'alpha': 6000}\n",
      "0.05314061851 {'alpha': 7000}\n",
      "0.0531427412412 {'alpha': 8000}\n"
     ]
    }
   ],
   "source": [
    "# fine tune\n",
    "param_ridge3 = {'alpha': [2000, 3000, 4000, 5000, 6000, 7000, 8000]}\n",
    "\n",
    "grid_ridge3 = GridSearchCV(ridge, param_grid=param_ridge3, cv=5,\n",
    "                         scoring='neg_mean_absolute_error')\n",
    "grid_ridge3.fit(features_transformed, labels)\n",
    "\n",
    "cv_ridge3 = grid_ridge3.cv_results_\n",
    "for mean_score, params in zip(cv_ridge3['mean_test_score'], \n",
    "                              cv_ridge3['params']):\n",
    "    print(-mean_score, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge(alpha=4000, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=8, solver='auto', tol=0.001)\n"
     ]
    }
   ],
   "source": [
    "# best alpha: 4000\n",
    "print(grid_ridge3.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0604260722443 {'n_neighbors': 5, 'p': 1}\n",
      "0.0604371977931 {'n_neighbors': 5, 'p': 2}\n",
      "0.0568401130065 {'n_neighbors': 10, 'p': 1}\n",
      "0.0569258934352 {'n_neighbors': 10, 'p': 2}\n",
      "0.0549210901831 {'n_neighbors': 20, 'p': 1}\n",
      "0.0549880686881 {'n_neighbors': 20, 'p': 2}\n"
     ]
    }
   ],
   "source": [
    "# grid search for KNN\n",
    "knn = KNeighborsRegressor(n_neighbors=5)\n",
    "param_knn = {'n_neighbors': [5, 10, 20], 'p': [1, 2]}\n",
    "\n",
    "grid_knn = GridSearchCV(knn, param_grid=param_knn, cv=3, n_jobs=2,\n",
    "                         scoring='neg_mean_absolute_error')\n",
    "grid_knn.fit(features_transformed, labels)\n",
    "\n",
    "cv_knn = grid_knn.cv_results_\n",
    "for mean_score, params in zip(cv_knn['mean_test_score'], \n",
    "                              cv_knn['params']):\n",
    "    print(-mean_score, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0549210901831 {'n_neighbors': 20}\n",
      "0.0541970097562 {'n_neighbors': 30}\n",
      "0.0538585365139 {'n_neighbors': 40}\n",
      "0.05366127579 {'n_neighbors': 50}\n"
     ]
    }
   ],
   "source": [
    "# fine tune\n",
    "knn = KNeighborsRegressor(n_neighbors=20, p=1)\n",
    "param_knn2 = {'n_neighbors': [20, 30, 40, 50]}\n",
    "\n",
    "grid_knn2 = GridSearchCV(knn, param_grid=param_knn2, cv=3, n_jobs=2,\n",
    "                         scoring='neg_mean_absolute_error')\n",
    "grid_knn2.fit(features_transformed, labels)\n",
    "\n",
    "cv_knn2 = grid_knn2.cv_results_\n",
    "for mean_score, params in zip(cv_knn2['mean_test_score'], \n",
    "                              cv_knn2['params']):\n",
    "    print(-mean_score, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05366127579 {'n_neighbors': 50}\n",
      "0.0535133545367 {'n_neighbors': 60}\n",
      "0.0534084607925 {'n_neighbors': 70}\n",
      "0.0533529750979 {'n_neighbors': 80}\n"
     ]
    }
   ],
   "source": [
    "# fine tune\n",
    "knn = KNeighborsRegressor(n_neighbors=50, p=1)\n",
    "param_knn3 = {'n_neighbors': [50, 60, 70, 80]}\n",
    "\n",
    "grid_knn3 = GridSearchCV(knn, param_grid=param_knn3, cv=3, n_jobs=2,\n",
    "                         scoring='neg_mean_absolute_error')\n",
    "grid_knn3.fit(features_transformed, labels)\n",
    "\n",
    "cv_knn3 = grid_knn3.cv_results_\n",
    "for mean_score, params in zip(cv_knn3['mean_test_score'], \n",
    "                              cv_knn3['params']):\n",
    "    print(-mean_score, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "          metric_params=None, n_jobs=1, n_neighbors=80, p=1,\n",
      "          weights='uniform')\n"
     ]
    }
   ],
   "source": [
    "# model\n",
    "print(grid_knn3.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0531147282249 {'max_depth': 3, 'n_estimators': 10}\n",
      "0.0530987405142 {'max_depth': 3, 'n_estimators': 50}\n",
      "0.053093382021 {'max_depth': 3, 'n_estimators': 100}\n",
      "0.0530525955749 {'max_depth': 5, 'n_estimators': 10}\n",
      "0.0530105185664 {'max_depth': 5, 'n_estimators': 50}\n",
      "0.0530089600657 {'max_depth': 5, 'n_estimators': 100}\n",
      "0.0532443393581 {'max_depth': 10, 'n_estimators': 10}\n",
      "0.0530095235023 {'max_depth': 10, 'n_estimators': 50}\n",
      "0.0529812476742 {'max_depth': 10, 'n_estimators': 100}\n"
     ]
    }
   ],
   "source": [
    "# grid search for RF\n",
    "rf = RandomForestRegressor(random_state=SEED)\n",
    "param_rf = {'n_estimators': [10, 50, 100], 'max_depth': [3, 5, 10]}\n",
    "\n",
    "grid_rf = GridSearchCV(rf, param_grid=param_rf, cv=5,\n",
    "                         scoring='neg_mean_absolute_error')\n",
    "grid_rf.fit(features_transformed, labels)\n",
    "\n",
    "cv_rf = grid_rf.cv_results_\n",
    "for mean_score, params in zip(cv_rf['mean_test_score'], \n",
    "                              cv_rf['params']):\n",
    "    print(-mean_score, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0530089600657 {'n_estimators': 100}\n",
      "0.0530076035873 {'n_estimators': 150}\n",
      "0.0530074511852 {'n_estimators': 200}\n"
     ]
    }
   ],
   "source": [
    "# fine tune\n",
    "rf = RandomForestRegressor(random_state=SEED, max_depth=5)\n",
    "param_rf2 = {'n_estimators': [100, 150, 200]}\n",
    "\n",
    "grid_rf2 = GridSearchCV(rf, param_grid=param_rf2, cv=5,\n",
    "                         scoring='neg_mean_absolute_error')\n",
    "grid_rf2.fit(features_transformed, labels)\n",
    "\n",
    "cv_rf2 = grid_rf2.cv_results_\n",
    "for mean_score, params in zip(cv_rf2['mean_test_score'], \n",
    "                              cv_rf2['params']):\n",
    "    print(-mean_score, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0530074511852 {'n_estimators': 200}\n",
      "0.0530045805246 {'n_estimators': 250}\n",
      "0.05300359793 {'n_estimators': 300}\n",
      "0.0530031392825 {'n_estimators': 350}\n"
     ]
    }
   ],
   "source": [
    "# fine tune\n",
    "rf = RandomForestRegressor(random_state=SEED, max_depth=5)\n",
    "param_rf3 = {'n_estimators': [200, 250, 300, 350]}\n",
    "\n",
    "grid_rf3 = GridSearchCV(rf, param_grid=param_rf3, cv=5, n_jobs=2,\n",
    "                         scoring='neg_mean_absolute_error')\n",
    "grid_rf3.fit(features_transformed, labels)\n",
    "\n",
    "cv_rf3 = grid_rf3.cv_results_\n",
    "for mean_score, params in zip(cv_rf3['mean_test_score'], \n",
    "                              cv_rf3['params']):\n",
    "    print(-mean_score, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0530031392825 {'n_estimators': 350}\n",
      "0.0530033507674 {'n_estimators': 400}\n",
      "0.0530024647723 {'n_estimators': 500}\n"
     ]
    }
   ],
   "source": [
    "# fine tune\n",
    "rf = RandomForestRegressor(random_state=SEED, max_depth=5)\n",
    "param_rf4 = {'n_estimators': [350, 400, 500]}\n",
    "\n",
    "grid_rf4 = GridSearchCV(rf, param_grid=param_rf4, cv=5, n_jobs=2,\n",
    "                         scoring='neg_mean_absolute_error')\n",
    "grid_rf4.fit(features_transformed, labels)\n",
    "\n",
    "cv_rf4 = grid_rf4.cv_results_\n",
    "for mean_score, params in zip(cv_rf4['mean_test_score'], \n",
    "                              cv_rf4['params']):\n",
    "    print(-mean_score, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=5,\n",
      "           max_features='auto', max_leaf_nodes=None,\n",
      "           min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "           n_estimators=500, n_jobs=1, oob_score=False, random_state=8,\n",
      "           verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "# model\n",
    "print(grid_rf4.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GradientBoosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0529321639709 {'max_depth': 3, 'n_estimators': 100}\n",
      "0.0529623922466 {'max_depth': 3, 'n_estimators': 150}\n",
      "0.0529861676242 {'max_depth': 3, 'n_estimators': 200}\n",
      "0.0530355975463 {'max_depth': 5, 'n_estimators': 100}\n",
      "0.0531170844951 {'max_depth': 5, 'n_estimators': 150}\n",
      "0.0532187494185 {'max_depth': 5, 'n_estimators': 200}\n",
      "0.0538949579796 {'max_depth': 10, 'n_estimators': 100}\n",
      "0.0542690280416 {'max_depth': 10, 'n_estimators': 150}\n",
      "0.0546415439229 {'max_depth': 10, 'n_estimators': 200}\n"
     ]
    }
   ],
   "source": [
    "# grid search for GB\n",
    "gb = GradientBoostingRegressor(random_state=SEED)\n",
    "param_gb = {'n_estimators': [100, 150, 200], 'max_depth': [3, 5, 10]}\n",
    "\n",
    "grid_gb = GridSearchCV(gb, param_grid=param_gb, cv=5, n_jobs=2,\n",
    "                         scoring='neg_mean_absolute_error')\n",
    "grid_gb.fit(features_transformed, labels)\n",
    "\n",
    "cv_gb = grid_gb.cv_results_\n",
    "for mean_score, params in zip(cv_gb['mean_test_score'], \n",
    "                              cv_gb['params']):\n",
    "    print(-mean_score, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0529322186146 {'n_estimators': 70}\n",
      "0.0529304416679 {'n_estimators': 80}\n",
      "0.0529312332548 {'n_estimators': 90}\n",
      "0.0529321639709 {'n_estimators': 100}\n"
     ]
    }
   ],
   "source": [
    "# fine tune\n",
    "gb = GradientBoostingRegressor(random_state=SEED, max_depth=3)\n",
    "param_gb2 = {'n_estimators': [70, 80, 90, 100]}\n",
    "\n",
    "grid_gb2 = GridSearchCV(gb, param_grid=param_gb2, cv=5, n_jobs=2,\n",
    "                         scoring='neg_mean_absolute_error')\n",
    "grid_gb2.fit(features_transformed, labels)\n",
    "\n",
    "cv_gb2 = grid_gb2.cv_results_\n",
    "for mean_score, params in zip(cv_gb2['mean_test_score'], \n",
    "                              cv_gb2['params']):\n",
    "    print(-mean_score, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
      "             learning_rate=0.1, loss='ls', max_depth=3, max_features=None,\n",
      "             max_leaf_nodes=None, min_impurity_split=1e-07,\n",
      "             min_samples_leaf=1, min_samples_split=2,\n",
      "             min_weight_fraction_leaf=0.0, n_estimators=80, presort='auto',\n",
      "             random_state=8, subsample=1.0, verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "# model\n",
    "print(grid_gb2.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CV - optimized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# models to test\n",
    "models = []\n",
    "models.append(('Ridge', Ridge(random_state=SEED, alpha=4000)))\n",
    "models.append(('KNN', KNeighborsRegressor(n_neighbors=80, p=1)))\n",
    "models.append(('RF', RandomForestRegressor(random_state=SEED, max_depth=5, n_estimators=500)))\n",
    "models.append(('GB', GradientBoostingRegressor(random_state=SEED, max_depth=3, n_estimators=80)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge: score mean -0.053128 (score std 0.000603)\n",
      "KNN: score mean -0.053374 (score std 0.000624)\n",
      "RF: score mean -0.053026 (score std 0.000639)\n",
      "GB: score mean -0.052970 (score std 0.000647)\n"
     ]
    }
   ],
   "source": [
    "# evaluate each model\n",
    "results = []\n",
    "names = []\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "\n",
    "for name, model in models:\n",
    "    cv_results = cross_val_score(model, features_transformed, labels, cv=kfold, \n",
    "                                 scoring='neg_mean_absolute_error')\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    print(\"%s: score mean %f (score std %f)\" % (name, cv_results.mean(), cv_results.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**THE END** "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
